import os
import json
import time
import threading
import collections
import datetime
import logging
import sys
from flask import Flask, jsonify
import pika
from prometheus_client import start_http_server, Counter, Gauge
from dotenv import load_dotenv
import redis

# Load environment variables for local development (if not running in Kubernetes)
load_dotenv()

# --- Configure Logging ---
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG) # Keep DEBUG for now
handler = logging.StreamHandler(sys.stdout)
handler.setLevel(logging.DEBUG) # Keep DEBUG for now
formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
handler.setFormatter(formatter)
logger.addHandler(handler)


app = Flask(__name__)

# --- Configuration ---
RABBITMQ_HOST = os.environ.get('RABBITMQ_HOST', 'rabbitmq-service')
RABBITMQ_PORT = int(os.environ.get('RABBITMQ_PORT', 5672))
RABBITMQ_USER = os.environ.get('RABBITMQ_USER', 'jdevlab')
RABBITMQ_PASS = os.environ.get('RABBITMQ_PASS', 'jdevlab')
RABBITMQ_QUEUE = os.environ.get('RABBITMQ_QUEUE', 'audit_events')
APP_PORT = int(os.environ.get('APP_PORT', 5001))
PROMETHEUS_PORT = int(os.environ.get('PROMETHEUS_PORT', 8001))

REDIS_HOST = os.environ.get('REDIS_HOST', 'redis-service')
REDIS_PORT = int(os.environ.get('REDIS_PORT', 6379))

# --- Prometheus Metrics ---
audit_analysis_processed_total = Counter('audit_analysis_processed_total', 'Total number of audit events processed.')
audit_analysis_alerts_total = Counter('audit_analysis_alerts_total', 'Total number of alerts generated by analysis.', ['alert_type', 'severity', 'user_id', 'server_hostname'])
rabbitmq_consumer_connection_status = Gauge('audit_analysis_rabbitmq_consumer_connection_status', 'Status of RabbitMQ consumer connection (1=connected, 0=disconnected).')
rabbitmq_messages_consumed_total = Counter('audit_analysis_rabbitmq_messages_consumed_total', 'Total number of messages consumed from RabbitMQ.')
redis_connection_status = Gauge('audit_analysis_redis_connection_status', 'Status of Redis connection (1=connected, 0=disconnected).')

# --- Analysis Rules Configuration ---
FAILED_LOGIN_WINDOW_SECONDS = 60
FAILED_LOGIN_THRESHOLD = 3
SENSITIVE_FILES = ["/etc/sudoers", "/root/.ssh/authorized_keys", "/etc/shadow", "/etc/passwd"]

# --- RabbitMQ Connection Management ---
connection = None
channel = None

# --- Redis Connection Management ---
redis_client = None

# --- Application Health State Variables (NEW!) ---
_redis_connected = False
_rabbitmq_connected = False
_health_lock = threading.Lock() # For thread-safe updates to health variables

def set_redis_status(status: bool):
    global _redis_connected
    with _health_lock:
        _redis_connected = status
    redis_connection_status.set(1 if status else 0)
    logger.debug(f"Redis connection status updated to: {status}. Prometheus Gauge set to {1 if status else 0}.") # Updated log

def set_rabbitmq_status(status: bool):
    global _rabbitmq_connected
    with _health_lock:
        _rabbitmq_connected = status
    rabbitmq_consumer_connection_status.set(1 if status else 0)
    logger.debug(f"RabbitMQ connection status updated to: {status}. Prometheus Gauge set to {1 if status else 0}.") # Updated log


def initialize_redis():
    global redis_client
    logger.info(f"Attempting to connect to Redis at {REDIS_HOST}:{REDIS_PORT}...")
    try:
        redis_client = redis.StrictRedis(host=REDIS_HOST, port=REDIS_PORT, decode_responses=True, socket_connect_timeout=5)
        redis_client.ping()
        set_redis_status(True) # Use the new setter
        logger.info("Successfully connected to Redis.")
        return True
    except redis.exceptions.ConnectionError as e:
        set_redis_status(False) # Use the new setter
        logger.error(f"Failed to connect to Redis (ConnectionError): {e}.", exc_info=True)
        redis_client = None
        return False
    except Exception as e:
        set_redis_status(False) # Use the new setter
        logger.exception(f"An unexpected error occurred during Redis connection: {type(e).__name__}: {e}.")
        redis_client = None
        return False

def connect_rabbitmq_consumer():
    global connection, channel
    logger.info(f"Attempting to connect to RabbitMQ at {RABBITMQ_HOST}:{RABBITMQ_PORT} as user '{RABBITMQ_USER}'...")
    try:
        credentials = pika.PlainCredentials(RABBITMQ_USER, RABBITMQ_PASS)
        parameters = pika.ConnectionParameters(RABBITMQ_HOST, RABBITMQ_PORT, '/', credentials, heartbeat=60)
        connection = pika.BlockingConnection(parameters)
        channel = connection.channel()

        try:
            channel.queue_declare(queue=RABBITMQ_QUEUE, durable=True)
            logger.info(f"Successfully declared queue '{RABBITMQ_QUEUE}'.")
        except pika.exceptions.ChannelClosedByBroker as e:
            logger.error(f"Queue declaration failed: Channel closed by broker during queue_declare for queue '{RABBITMQ_QUEUE}': {e}.")
            set_rabbitmq_status(False) # Use the new setter
            if connection and connection.is_open:
                connection.close()
            connection = None
            channel = None
            return False
        except Exception as e:
            logger.exception(f"An unexpected error occurred during queue_declare for queue '{RABBITMQ_QUEUE}': {type(e).__name__}: {e}.")
            set_rabbitmq_status(False) # Use the new setter
            if connection and connection.is_open:
                connection.close()
            connection = None
            channel = None
            return False

        set_rabbitmq_status(True) # Use the new setter
        logger.info(f"Successfully connected to RabbitMQ at {RABBITMQ_HOST}:{RABBITMQ_PORT}.")
        return True
    except pika.exceptions.AMQPConnectionError as e:
        set_rabbitmq_status(False) # Use the new setter
        logger.error(f"Failed to connect to RabbitMQ (AMQPConnectionError): {e}.", exc_info=True)
        connection = None
        channel = None
        return False
    except pika.exceptions.ChannelClosedByBroker as e:
        set_rabbitmq_status(False) # Use the new setter
        logger.error(f"RabbitMQ Channel Closed by Broker (general): {e}.", exc_info=True)
        connection = None
        channel = None
        return False
    except Exception as e:
        set_rabbitmq_status(False) # Use the new setter
        logger.exception(f"An unexpected error occurred during RabbitMQ connection: {type(e).__name__}: {e}.")
        connection = None
        channel = None
        return False


def on_message_callback(ch, method, properties, body):
    global audit_analysis_processed_total, audit_analysis_alerts_total, rabbitmq_messages_consumed_total, redis_client
    try:
        event = json.loads(body.decode('utf-8'))
        logger.info(f"Received event: {event.get('event_type')} from {event.get('server_hostname')} (Event ID: {event.get('event_id')})")
        audit_analysis_processed_total.inc()
        rabbitmq_messages_consumed_total.inc()

        # --- Rule 1: Failed Login Attempts (Using Redis Sorted Set) ---
        if event.get('event_type') == 'user_login' and event.get('action_result') == 'FAILURE':
            if not redis_client:
                logger.error("Redis client is not initialized or connected within on_message_callback. Skipping failed login analysis for this event. Requeuing message.")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
                return

            user_id = event.get('user_id', 'unknown')
            server_hostname = event.get('server_hostname', 'unknown')
            current_unix_timestamp = time.time()
            current_iso_timestamp = datetime.datetime.utcnow().isoformat() + "Z"

            zset_key = f"failed_logins_zset:{user_id}:{server_hostname}"

            try:
                pipe = redis_client.pipeline()
                pipe.zadd(zset_key, {current_iso_timestamp: current_unix_timestamp})
                pipe.zremrangebyscore(zset_key, 0, current_unix_timestamp - FAILED_LOGIN_WINDOW_SECONDS)
                pipe.zcard(zset_key)
                pipe.expire(zset_key, FAILED_LOGIN_WINDOW_SECONDS + 60) # Set expiry on the key itself
                results = pipe.execute()
                current_attempts_in_window = results[-1]

                logger.debug(f"User '{user_id}' on '{server_hostname}': {current_attempts_in_window} failed attempts in window (Redis ZSET). Redis pipe execution successful.")

                if current_attempts_in_window >= FAILED_LOGIN_THRESHOLD:
                    alert_message = (f"!!!! ALERT: {FAILED_LOGIN_THRESHOLD} or more failed login attempts for user "
                                     f"'{user_id}' on '{server_hostname}' within {FAILED_LOGIN_WINDOW_SECONDS} seconds (Redis)!")
                    logger.warning(alert_message)
                    audit_analysis_alerts_total.labels(
                        alert_type='failed_login_burst',
                        severity='CRITICAL',
                        user_id=user_id,
                        server_hostname=server_hostname
                    ).inc()
            except redis.exceptions.ConnectionError as e:
                set_redis_status(False) # Use the new setter
                logger.error(f"Redis connection error during failed login analysis in on_message_callback: {e}.", exc_info=True)
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
                return
            except Exception as e:
                logger.exception(f"Unexpected error in Redis-based failed login analysis for user '{user_id}': {e}. Requeuing message.")
                ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)
                return


        # --- Rule 2: Critical File Modifications ---
        if event.get('event_type') == 'file_modified' and event.get('action_result') == 'MODIFIED':
            resource = event.get('resource', '')
            if any(sensitive_file in resource for sensitive_file in SENSITIVE_FILES):
                alert_message = (f"!!!! ALERT: Sensitive file '{resource}' modified by '{event.get('user_id')}' "
                                 f"on '{event.get('server_hostname')}'!")
                logger.critical(alert_message)
                audit_analysis_alerts_total.labels(
                    alert_type='sensitive_file_modified',
                    severity='CRITICAL',
                    user_id=event.get('user_id', 'unknown'),
                    server_hostname=event.get('server_hostname', 'unknown')
                ).inc()

        ch.basic_ack(delivery_tag=method.delivery_tag)
        logger.debug(f"Message {method.delivery_tag} acknowledged.")

    except json.JSONDecodeError as e:
        logger.error(f"Error decoding JSON message: {e} - Body: {body.decode('utf-8', errors='ignore')}. Not requeuing.")
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=False)
    except Exception as e:
        logger.exception(f"Error processing event outside specific rule: {e} - Event body: {body.decode('utf-8', errors='ignore')}. Requeuing.")
        ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)


def start_consumer():
    global connection, channel
    logger.info("Consumer thread started. Entering connection loop.")
    while True:
        logger.debug("Consumer loop: Checking Redis connection.")
        if not initialize_redis():
            logger.error("Consumer loop: Redis not connected. Cannot proceed to RabbitMQ consumer. Retrying Redis in 5 seconds...")
            time.sleep(5)
            continue
        logger.debug("Consumer loop: Redis is connected. Checking RabbitMQ connection.")
        if not connect_rabbitmq_consumer():
            logger.info("Consumer loop: RabbitMQ connection failed, retrying in 5 seconds...")
            time.sleep(5)
            continue
        
        logger.info("Consumer loop: Both Redis and RabbitMQ connections are established. Attempting to start consuming.")
        try:
            channel.basic_consume(queue=RABBITMQ_QUEUE, on_message_callback=on_message_callback, auto_ack=False)
            logger.info(f"Successfully set up basic_consume for queue '{RABBITMQ_QUEUE}'.")

            logger.info("Connection successful. Starting to consume messages...")
            # This is a blocking call. Control returns here only on disconnect or error.
            channel.start_consuming()
            logger.info("Consumer stopped consuming (likely connection lost or error). Re-entering connection loop.")
        except pika.exceptions.AMQPConnectionError:
            logger.error("Consumer loop: Lost RabbitMQ connection (AMQPConnectionError), attempting to reconnect.")
            set_rabbitmq_status(False) # Use the new setter
            connection = None
            channel = None
            time.sleep(5)
        except KeyboardInterrupt:
            logger.info("Consumer loop: Stopping consumer due to KeyboardInterrupt.")
            if connection and connection.is_open:
                connection.close()
            break
        except Exception as e:
            logger.exception(f"Consumer loop: An unexpected error occurred in consumer loop: {type(e).__name__}: {e}. Attempting to reconnect.")
            set_rabbitmq_status(False) # Use the new setter
            if connection and connection.is_open:
                connection.close()
            connection = None
            channel = None
            time.sleep(5)


# --- Flask API Endpoints ---
@app.route('/healthz')
def health_check():
    # Read the status from our new health variables (protected by lock)
    with _health_lock:
        rabbitmq_ok = _rabbitmq_connected
        redis_ok = _redis_connected
    
    # Optional: Read raw gauge values for comparison, but don't rely on them for logic
    rabbitmq_gauge_value_raw = "N/A"
    redis_gauge_value_raw = "N/A"
    try:
        rabbitmq_gauge_value_raw = rabbitmq_consumer_connection_status._value
        redis_gauge_value_raw = redis_connection_status._value
    except AttributeError:
        # Handle cases where _value might not exist or be directly readable
        pass

    logger.debug(f"Health check: Raw gauge values (for info): RabbitMQ={rabbitmq_gauge_value_raw}, Redis={redis_gauge_value_raw}")
    logger.debug(f"Health check: Status based on internal flags: RabbitMQ Connected: {rabbitmq_ok}, Redis Connected: {redis_ok}")

    status = "healthy" if rabbitmq_ok and redis_ok else "unhealthy"
    is_consumer_thread_alive = consumer_thread.is_alive() if 'consumer_thread' in globals() else False

    logger.debug(f"Health check requested. Overall Status: {status}, Consumer Thread Alive: {is_consumer_thread_alive}")

    return jsonify({
        "status": status,
        "rabbitmq_connected": rabbitmq_ok,
        "redis_connected": redis_ok,
        "consumer_thread_alive": is_consumer_thread_alive
    }), 200 if rabbitmq_ok and redis_ok else 503

@app.route('/metrics')
def prometheus_metrics():
    from prometheus_client import generate_latest
    return generate_latest(), 200

# --- Main Application Start ---
if __name__ == '__main__':
    logger.info("Starting Audit Log Analysis Service...")

    try:
        start_http_server(PROMETHEUS_PORT)
        logger.info(f"Prometheus metrics server started on port {PROMETHEUS_PORT}")
    except Exception as e:
        logger.critical(f"FATAL: Could not start Prometheus metrics server: {e}", exc_info=True)
        sys.exit(1)

    # Initial Redis connection check - will be retried by consumer thread as well
    # This also sets the initial _redis_connected flag and Prometheus gauge
    if not initialize_redis():
        logger.critical("FATAL: Initial Redis connection failed from main thread. Consumer thread will retry. Continuing startup.")

    consumer_thread = threading.Thread(target=start_consumer, daemon=True)
    try:
        consumer_thread.start()
        logger.info("RabbitMQ consumer thread started.")
    except Exception as e:
        logger.critical(f"FATAL: Could not start RabbitMQ consumer thread: {e}", exc_info=True)
        sys.exit(1)

    logger.info(f"Starting Flask application on 0.0.0.0:{APP_PORT}...")
    try:
        app.run(host='0.0.0.0', port=APP_PORT)
        logger.info("Flask application stopped cleanly.")
    except Exception as e:
        logger.critical(f"FATAL: Flask application crashed unexpectedly: {e}", exc_info=True)
        sys.exit(1)

    logger.info("Main application process exiting.")
# infra/context/CONTEXT.md

## AuditFlow Platform Infrastructure Context

This document provides a comprehensive overview of the AuditFlow Platform's underlying infrastructure. Its purpose is to give a holistic understanding of the environment, architecture, and core components for both human developers and AI models (LLMs) requiring full context for operations, development, or analysis.

For tool-specific details, refer to the following context files:
* **[Ansible Context](./CONTEXT_ANSIBLE.md)**
---

### 1. Project Overview & Infrastructure Goals

* **Project Purpose:** The AuditFlow Platform is designed to collect, process, analyze, and notify about audit events generated by various microservices. It aims to provide a robust and scalable solution for audit trail management.
* **Primary Infrastructure Goal:** To establish and maintain a cost-effective, automated continuous deployment (CD) pipeline to a resilient home lab Kubernetes (K3s) cluster using self-hosted GitHub Actions runners. This enables rapid and consistent delivery of microservices.
* **Key Infrastructure Principles:**
    * **Infrastructure as Code (IaC):** All infrastructure components are defined, provisioned, and managed through code, version-controlled in Git.
    * **Kubernetes-Native Deployments:** Microservices are deployed as native Kubernetes workloads, leveraging its orchestration capabilities.
    * **Automation-First:** Manual intervention is minimized through extensive use of automation tools like Ansible and GitHub Actions.
    * **Cost-Effective Home Lab Utilization:** Leveraging Raspberry Pi hardware for development and staging environments to optimize costs.

### 2. Infrastructure Architecture Overview

#### A. Hardware Layer

* **Type of Devices:** 3 x Raspberry Pi 5.
* **Current/Planned Configuration:**
    * **Master Node:** 1x Raspberry Pi 5 (named `pi3.jdevlab.local`, IP `192.168.1.161`) serving as the K3s cluster master.
    * **Worker Node(s):** 1x Raspberry Pi 5 (named `pi1.jdevlab.local`, IP `192.168.1.160`) currently serving as a K3s worker node and the primary host for the self-hosted GitHub Actions runner deployment. Future plans include adding another Raspberry Pi 5 worker.
* **Physical Location:** All hardware resides within a personal Home Lab environment.

#### B. Networking

* **IP Address Strategy:**
    * Internal Static IPs: All Raspberry Pis are assigned static IP addresses within the home lab's local network (e.g., `192.168.1.0/24`).
    * Internal DNS: A local DNS server resolves `.jdevlab.local` domain names (e.g., `pi1.jdevlab.local`) to their corresponding internal static IPs, ensuring consistent host resolution for Ansible and intra-cluster communication.
    * Dynamic Public IP: The home lab router uses a Dynamic DNS (DDNS) service to keep its public IP updated, primarily for external Git/GitHub communication.
* **Internal Network Layout:** A flat network configuration on the `192.168.1.0/24` subnet.
* **External Access:** Limited external access is configured. There is no direct external access to internal services or K3s nodes. Communication with GitHub Actions occurs outbound from the self-hosted runner.

#### C. Container Orchestration Layer (K3s)

* **Cluster Distribution:** K3s (lightweight, CNCF certified Kubernetes distribution).
* **K3s Version:** Aiming for a recent stable version (e.g., `v1.28.x`). The installation script targets the latest stable release by default unless specified.
* **Container Runtime:** `containerd` (K3s's default runtime). It is explicitly stated that the Docker daemon (`dockerd`) is **NOT** installed on any K3s cluster nodes to avoid resource contention and conflicts with `containerd`.
* **Networking Plugin (CNI):** Flannel (K3s's default CNI plugin) for pod networking.
* **Storage (CSI):** K3s's default `local-path` provisioner is actively used for basic persistent volume claims (PVCs). This provides node-local persistent storage for stateful applications like PostgreSQL and the GitHub Actions runner. Future considerations may include a more robust shared storage solution like Longhorn or NFS for production-like environments.

#### D. Core Application Dependencies (Deployed on K3s)

The following essential application dependencies are deployed and managed as Kubernetes workloads within the K3s cluster:

* **PostgreSQL:** Serves as the primary data store for the `notification-service` and other microservices. Utilizes a Persistent Volume Claim for data durability.
* **RabbitMQ:** Functions as the central message queue for asynchronous event processing and inter-service communication within the AuditFlow platform.
* **Redis:** Utilized for caching mechanisms and session management, particularly for the `audit-log-analysis` service, to enhance performance.
* **Deployment Strategy:** These core services are deployed using standard Kubernetes Deployments/StatefulSets, typically managed via Helm charts or Kustomize overlays.

#### E. CI/CD Infrastructure

* **Orchestrator:** GitHub Actions is the chosen platform for orchestrating all CI/CD workflows.
* **Runner Strategy:** A hybrid approach is employed:
    * **GitHub-Hosted Runners:** Used for Continuous Integration (CI) tasks, including code linting, testing, and crucially, building Docker images for microservices and pushing them to Docker Hub.
    * **Self-Hosted Runners:** Dedicated runners deployed within the home lab K3s cluster for Continuous Deployment (CD). These runners handle the actual deployment of pre-built Docker images to the K3s cluster.
* **Self-Hosted Runner Placement:** The self-hosted runner is deployed as a Kubernetes Deployment (a Pod running the runner agent) directly inside the K3s cluster. It leverages Kubernetes's resilience and resource management. Its work directory (`/home/runner/_work`) is persisted using a **PersistentVolumeClaim (PVC)** backed by the `local-path` storage class.
* **Self-Hosted Runner Container Execution:** The runner container's primary process is explicitly set in the Deployment to execute `/home/runner/actions-runner/run.sh`. This ensures the runner agent remains active and continuously listens for jobs from GitHub, preventing premature container exits (`CrashLoopBackOff`). Runner instances are dynamically named using the Kubernetes Downward API (e.g., `auditflow-k3s-runner-<POD_NAME>`).
* **Self-Hosted Runner Capabilities:** The container image for the self-hosted runner is provisioned to include `git` (for repository cloning), `kubectl` (for interacting with the K3s API), `helm` (for `dev`/`staging` deployments), and `kustomize` (for `prod`/`preprod` deployments). It **does NOT require a Docker daemon** as image building is offloaded to GitHub-hosted runners.

#### F. Infrastructure as Code (IaC) Tools

* **Ansible:** The primary IaC tool. It is responsible for:
    * Initial provisioning and configuration of Raspberry Pi hosts for K3s.
    * Installation and setup of K3s master and worker components.
    * Deployment of the self-hosted GitHub Actions runner *as Kubernetes manifests* into the K3s cluster.
    * **Full Lifecycle Management**: Ansible handles the complete lifecycle of the self-hosted GitHub Actions runner, including **initial deployment, updates, pre-deployment cleanup (deleting old runners from both K8s and GitHub's API), and post-deployment verification of its 'online' and 'idle' status directly via the GitHub API.**
* **Terraform/CloudFormation/ARM:** Currently not actively used for this specific home lab K3s cluster setup. These tools are reserved for potential future provisioning of cloud resources (e.g., VMs, networking, managed services) if the project expands beyond the home lab into a public cloud environment.

### 3. Environments & Deployment Strategy

* **Defined Environments:** The AuditFlow platform utilizes four distinct environments:
    * `dev`: Development environment, typically ephemeral or frequently updated.
    * `staging`: A pre-production environment for integration testing and quality assurance.
    * `preprod`: A near-production environment for final validation before `prod`.
    * `prod`: The live production environment.
* **Mapping to Kubernetes:**
    * For `dev` and `staging`, separate namespaces within the same K3s cluster are used to isolate deployments.
    * For `preprod` and `prod`, future plans may involve separate K3s clusters or dedicated namespaces with stricter access controls and resource quotas.
* **Deployment Tools per Environment:**
    * **Helm:** Used for deployments to `dev` and `staging` environments, leveraging its templating and package management capabilities.
    * **Kustomize:** Preferred for `prod` and `preprod` environments to apply declarative configuration overlays and promote consistent, immutable deployments.
* **Deployment Triggers:** Automated GitHub Actions workflows trigger deployments:
    * Pushes to the `dev` branch typically trigger `dev` environment deployments.
    * Pushes to `release/*` branches or specific tags might trigger `staging` or `preprod` deployments.
    * Pushes to the `main` branch (after successful testing/validation) trigger `prod` deployments.

### 4. Security Considerations (Infrastructure-Level)

* **SSH Access:** All automated and direct host access to Raspberry Pi nodes is secured using SSH key-based authentication. Password-based SSH access is disabled or severely restricted where possible for automation.
* **Secrets Management:**
    * **Ansible Vault:** Used to encrypt sensitive credentials and configuration data within the Infrastructure as Code repository (e.g., SSH user passwords, sudo passwords, **GitHub Personal Access Tokens for API interactions**).
    * **GitHub Secrets:** Utilized within GitHub Actions workflows to securely store and inject sensitive information (e.g., GitHub Personal Access Tokens for runner registration, Docker Hub credentials) during CI/CD pipeline execution.
    * **Kubernetes Secrets:** Used for managing application-level sensitive data (e.g., database credentials, API keys) within the K3s cluster, accessible by application pods.
* **Firewall Rules:** The default Raspberry Pi OS firewall (UFW) is configured to explicitly allow necessary inbound traffic, including SSH (port 22) and K3s API traffic (port 6443), while blocking unnecessary ports.

### 5. Monitoring, Logging, and Alerts (Infrastructure-Level)

* **Current Strategy:** Basic systemd journal logging for K3s services and host components.
* **Future Plans:** Implement a comprehensive monitoring stack using Prometheus for metrics collection and Grafana for visualization. Loki will be integrated for centralized log aggregation from Kubernetes pods and host systems. Alerting will be configured via Alertmanager.

### 6. Assumptions & Constraints

* **Home Lab Environment:** This infrastructure operates within a home lab context, implying potential limitations compared to enterprise-grade environments (e.g., reliance on consumer-grade network equipment, potential for internet outages, no guaranteed 24/7 uptime for non-critical services).
* **Assumed User Permissions:** All automation (Ansible) and direct SSH access assumes a non-root user (e.g., `jdevlab`) with configured `sudo` privileges (passwordless sudo for automation is preferred where possible, or `ansible_become_pass` is used).
* **Ansible Control Machine:** The machine executing Ansible playbooks is assumed to have Python 3.12 installed (or compatible) and uses Poetry for managing Python dependencies (including Ansible itself), even if `poetry run` isn't directly used for execution. Network connectivity from the control machine to all Raspberry Pi nodes is required.
* **Dynamic IP Handling:** The external facing IP of the home lab may change, requiring reliance on DDNS for external services and internal DNS for consistent host resolution.
# CONTEXT.md

## Project Summary

**Project Genesis** is a comprehensive, hands-on learning initiative designed to build and manage a multi-service application within a modern **DevOps ecosystem**. It serves as a real-world sandbox demonstrating best practices across **DevOps, Site Reliability Engineering (SRE), DevSecOps, and FinDevOps**. The core application, the **AuditFlow Platform**, is orchestrated on a **Kubernetes (K3s)** cluster, showcasing the entire software delivery lifecycle from infrastructure provisioning and **full lifecycle management of self-hosted runners** to application deployment and ongoing operations, all managed as code.

---

## Project Goals

The primary goals of Project Genesis are to provide practical experience in:
* Designing and deploying the **AuditFlow Platform** in a Kubernetes environment.
* Implementing **Infrastructure as Code (IaC)** for consistent and automated infrastructure provisioning, including full lifecycle management of critical components.
* Building robust **CI/CD pipelines** for continuous integration and delivery.
* Managing Kubernetes resources effectively using **Helm and Kustomize**.
* Integrating **testing and code quality** into the development workflow.
* Establishing **observability** for monitoring application health and performance.
* Applying **DevSecOps principles**, including Static Application Security Testing (SAST) and Software Composition Analysis (SCA).
* Exploring **FinDevOps concepts** for cost-effective cloud-native operations.

---

## Frameworks Used

* **Kubernetes (K3s)**: Lightweight Kubernetes distribution for container orchestration.
* **Python (Flask/other microframeworks)**: Primary language for application services.
* **Docker**: Containerization platform.
* **GitHub Actions**: CI/CD automation.
* **Terraform**: Infrastructure as Code for provisioning.
* **Ansible**: Configuration management, K3s cluster setup, and **full lifecycle management of GitHub Actions runners including API interaction**.
* **Helm**: Kubernetes package manager.
* **Kustomize**: Kubernetes configuration customization.
* **Pytest**: Python testing framework.
* **Prometheus & Grafana**: Monitoring and alerting (planned/integrated).
* **Bandit**: Static Application Security Testing (SAST) for Python.
* **Trivy**: Software Composition Analysis (SCA) and vulnerability scanning.

---

## Tech Stack Overview

* **Languages:** Python
* **Containerization:** Docker
* **Orchestration:** Kubernetes (K3s)
* **CI/CD:** GitHub Actions
* **Infrastructure as Code:** Terraform, Ansible
* **Databases/Messaging:** PostgreSQL (persistent data), RabbitMQ (message broker), Redis (caching/temporary data)
* **Security Tools:** Bandit (SAST), Trivy (SCA)
* **Monitoring (Planned/Integrated):** Prometheus, Grafana

---

### Infrastructure as Code (IaC) Tools
* **Ansible:** The primary IaC tool. It is responsible for:
    * Initial provisioning and configuration of Raspberry Pi hosts for K3s.
    * Installation and setup of K3s master and worker components.
    * Deployment of the self-hosted GitHub Actions runner *as Kubernetes manifests* into the K3s cluster.
    * **Full Lifecycle Management**: Ansible handles the complete lifecycle of the self-hosted GitHub Actions runner, including **initial deployment, updates, pre-deployment cleanup (deleting old runners from both K8s and GitHub's API), and post-deployment verification of its 'online' and 'idle' status directly via the GitHub API.**

---

## Application Type

This project is a **web application** composed of multiple interconnected **microservices**. It includes both backend services (e.g., `audit-log-analysis`, `notification-service`) and a frontend dashboard (e.g., `event-audit-dashboard`).

---

## Directory Structure
This project is a dynamic and evolving learning initiative. Its status and ongoing development can be tracked via the [GitHub Project board](https://github.com/jojees/project-genesis/projects) and [GitHub Issues](https://github.com/jojees/project-genesis/issues).

The project is meticulously organized to separate application code from infrastructure, documentation, and CI/CD configurations. This structure promotes clarity, maintainability, and scalability.
```
.
‚îú‚îÄ‚îÄ .github/                      # GitHub Actions workflows for CI/CD
‚îú‚îÄ‚îÄ docs/                         # Project documentation (architecture, pillars, setup guides)
‚îú‚îÄ‚îÄ infra/                        # Infrastructure as Code (Terraform for provisioning, Ansible for configuration)
‚îÇ   ‚îú‚îÄ‚îÄ terraform/                # Terraform configurations for base infrastructure
‚îÇ   ‚îî‚îÄ‚îÄ ansible/                  # Ansible playbooks and roles for configuration management (e.g., K3s installation)
‚îú‚îÄ‚îÄ k8s/                          # Kubernetes manifests and Helm charts
‚îÇ   ‚îú‚îÄ‚îÄ base/                     # Base Kubernetes YAMLs for individual services (can be integrated into Helm)
‚îÇ   ‚îú‚îÄ‚îÄ charts/                   # Helm charts for the entire application and individual services
‚îÇ   ‚îî‚îÄ‚îÄ overlays/                 # Kustomize overlays for environment-specific configurations
‚îú‚îÄ‚îÄ monitoring/                   # Configurations for Prometheus, Grafana, and alert rules
‚îú‚îÄ‚îÄ reports/                      # Generated reports (e.g., test coverage XML)
‚îú‚îÄ‚îÄ scripts/                      # Helper scripts for deployment, setup, etc.
‚îî‚îÄ‚îÄ src/                          # Application source code (the 'AuditFlow Platform' microservices)
‚îú‚îÄ‚îÄ audit_event_generator/
‚îú‚îÄ‚îÄ audit-log-analysis/
‚îú‚îÄ‚îÄ event-audit-dashboard/
‚îî‚îÄ‚îÄ notification-service/
‚îî‚îÄ‚îÄ tests/                # Unit and integration tests for each service
‚îú‚îÄ‚îÄ .gitignore                    # Files and directories to ignore in Git
‚îú‚îÄ‚îÄ .pytest.ini                   # Global Pytest configuration
‚îú‚îÄ‚îÄ CODE_OF_CONDUCT.md
‚îú‚îÄ‚îÄ CONTRIBUTING.md
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ README.md                     # This file
‚îî‚îÄ‚îÄ TODO.md                       # Project tasks and notes
```
For a more detailed breakdown of the directory contents, refer to the `docs/` directory.

**üé• Want to see Project Genesis in action and follow its development?** Check out our video tutorials and development logs on our [YouTube Channel](https://www.youtube.com/@JojeesDevOpsStudio)!

---

## Main Entry Point or App Initializer

For each Python microservice within the `src/` directory, the main entry points are typically:
* `src/*/app.py` (e.g., `src/audit-event-generator/app.py`, `src/event-audit-dashboard/event_audit_dashboard/app.py`)
* `src/*/main.py` (e.g., `src/audit-log-analysis/audit_analysis/main.py`, `src/notification-service/notification_service/main.py`)

These files initialize the respective Flask applications, set up logging, connect to message brokers (RabbitMQ), and interact with databases (Postgres, Redis).

---

## Setup Instructions (High-Level)

The project emphasizes Infrastructure as Code (IaC) and automated deployments. Key setup steps include:

1.  **Prerequisites:** Install Git, Docker, Python 3.9+ with Poetry, Terraform CLI, Ansible, `kubectl`, and Helm CLI.
2.  **Infrastructure Provisioning:**
    * Review and adjust **Terraform configurations** in `infra/terraform/`.
    * Initialize and apply Terraform: `terraform init`, `terraform plan`, `terraform apply -auto-approve`.
    * Configure the **K3s cluster** on provisioned nodes using **Ansible**, and **deploy/manage the self-hosted GitHub Actions runner**: `ansible-playbook -i infra/ansible/inventory.ini infra/ansible/homelab.yaml` followed by `ansible-playbook -i infra/ansible/inventory.ini infra/ansible/deploy_github_runner.yaml`.
3.  **Application Deployment:**
    * **Docker Images:** Built and pushed automatically to Docker Hub via GitHub Actions.
    * **Kubernetes Deployment:** Deploy the AuditFlow Platform using **Helm**. Navigate to the main Helm chart directory (e.g., `k8s/charts/events-app`), update dependencies (`helm dependency update`), and install (`helm install auditflow-platform . --namespace auditflow-platform --create-namespace -f values.yaml`).

Detailed instructions are available in the `docs/` directory, particularly `docs/setup.md`.

---

## Usage Instructions

Once deployed to Kubernetes:
* The **`event-audit-dashboard`** service provides a user interface to visualize audit events and alerts. Access will depend on Kubernetes service exposure (e.g., NodePort, LoadBalancer, Ingress).
* Other services (e.g., `audit-event-generator`, `audit-log-analysis`, `notification-service`) run as background processes, interacting via RabbitMQ and storing data in PostgreSQL or Redis.
* Monitoring tools (Prometheus, Grafana) would be used to observe application health and performance.

---

## Architecture Overview

The `AuditFlow Platform` is an **event-driven, microservice-based application**.
* **`audit_event_generator`**: Produces synthetic audit events.
* **`audit-log-analysis`**: Consumes events from RabbitMQ, performs analysis, and stores results (likely in Postgres or Redis).
* **`event-audit-dashboard`**: A web interface that fetches and displays audit events/alerts.
* **`notification-service`**: Handles alerts and notifications based on analysis outcomes.
* **`PostgreSQL`**: Relational database for persistent storage.
* **`RabbitMQ`**: Central message broker facilitating asynchronous communication between services.
* **`Redis`**: Key-value store for caching or temporary data.

Services communicate primarily via **RabbitMQ**, enabling a decoupled and scalable system. Kubernetes orchestrates these services, ensuring their availability and managing resources.

---

## Explanation of Key Modules/Components

* **`audit-event-generator` (Python Microservice):** Responsible for creating and publishing synthetic audit events to RabbitMQ.
* **`audit-log-analysis` (Python Microservice):** Consumes audit events from RabbitMQ, processes them (e.g., parsing, categorization), and stores the analyzed data. It likely interacts with Redis for temporary data and Postgres for persistent storage.
* **`event-audit-dashboard` (Python Web Application):** Provides a Flask-based web interface for users to visualize the processed audit events and alerts. It retrieves data from the backend services or databases.
* **`notification-service` (Python Microservice):** Subscribes to specific event streams from RabbitMQ or listens for triggers from the `audit-log-analysis` service, sending out notifications (e.g., email, alerts) based on defined rules. It interacts with Postgres for notification-related data.
* **`k8s/base`:** Contains the foundational Kubernetes YAML manifests for each service and infrastructure component (Postgres, RabbitMQ, Redis). These define Deployments, Services, StatefulSets, and PVCs.
* **`infra/terraform`:** Holds Terraform configurations to provision the underlying cloud infrastructure (e.g., VMs for K3s nodes) required for the Kubernetes cluster.
* **`infra/ansible`:** Contains Ansible playbooks to automate the installation and configuration of the K3s cluster on the provisioned infrastructure, **including the full lifecycle management of the self-hosted GitHub Actions runner.**
* **`docs/`:** Comprehensive documentation covering architecture, DevOps pillars, setup guides, and more.

---

## List of Dependencies and Scripts (Examples)

Dependencies for each Python service (`audit-event-generator`, `audit-log-analysis`, `event-audit-dashboard`, `notification-service`) are managed using `pyproject.toml` and `requirements.txt`. While specific contents vary per service, they typically include:

* **`Flask`**: For building web services.
* **`Pika`**: Python client for RabbitMQ.
* **`SQLAlchemy` / `Psycopg2`**: For PostgreSQL interaction.
* **`Redis`**: Python client for Redis.
* **`Gunicorn`**: WSGI HTTP Server.
* **`python-dotenv`**: For managing environment variables.
* **`pytest`**: For testing.
* **`bandit`**: For SAST.
* **`poetry`**: Python packaging and dependency management.

**Example from `src/audit-event-generator/pyproject.toml` (or similar for other services):**

```toml
[tool.poetry.dependencies]
python = "^3.9"
Flask = "^2.2.0"
pika = "^1.3.0"
python-dotenv = "^1.0.0"
gunicorn = "^20.1.0"
```

**Example from src/audit-event-generator/requirements.txt (auto-generated by poetry or manually maintained):**

```Flask==2.2.0
pika==1.3.0
python-dotenv==1.0.0
gunicorn==20.1.0
```
**Common scripts (conceptual, based on README.md and project structure):**

* `terraform init/plan/apply`: For infrastructure provisioning (infra/terraform).
* `ansible-playbook`: For K3s cluster setup and **GitHub Actions runner lifecycle management** (infra/ansible).
* `docker build/push`: For building and pushing service images (automated via GitHub Actions).
* `helm install/upgrade`: For deploying applications to Kubernetes (k8s/charts).
* `pytest`: For running unit/integration tests within src/*/tests/.
* `bandit`: For running SAST scans.
* `trivy fs / trivy image`: For SCA and vulnerability scanning.

---

## CI/CD Pipeline Details

The project leverages **GitHub Actions** for its **Continuous Integration and Continuous Delivery (CI/CD)** pipelines, specifically designed for Python microservices and Docker image management. The workflows are structured for efficiency, reusability, and robust security integration.

### Key Workflows:

* **`.github/workflows/build-python-services.yml` (Main CI Workflow):**
    * **Trigger:** Automatically runs on `push` events to `main`, `staging`, and `dev` branches.
    * **Optimized Builds:** Utilizes `dorny/paths-filter@v3` to detect changes in specific service directories (`src/event-audit-dashboard/`, `src/notification-service/`, `src/audit-log-analysis/`, `src/audit-event-generator/`).
    * **Conditional Execution:** Only triggers build and scan jobs for services that have undergone changes, optimizing pipeline execution time and resource usage.
    * **Reusable Workflow Calls:** Calls the `build-single-service.yml` reusable workflow for each modified service, passing necessary parameters like `service_name`, Docker credentials, and Git metadata.

* **`.github/workflows/build-single-service.yml` (Reusable Workflow):**
    * **Purpose:** Encapsulates the logic for building, pushing Docker images, and performing security scans for a *single* Python microservice. This promotes reusability and reduces duplication across multiple service pipelines.
    * **Inputs:** Requires `service_name`, `docker_org`, `docker_repo_prefix`, `ref_name` (branch name), `github_sha` (commit SHA), and Docker Hub credentials as secrets.
    * **Multi-architecture Builds:** Configured to build Docker images for `linux/amd64` and `linux/arm64` platforms, ensuring broad compatibility.
    * **Image Tagging:** Automatically generates Docker image tags based on the commit SHA, branch name (e.g., `main` gets `latest`, `dev` gets `dev`, `dev-snapshot`), and a combination of `docker_org` and `service_name`.
    * **Caching:** Uses GitHub Actions cache (`type=gha`) to speed up Docker image builds.

### Continuous Delivery (CD) Workflow

* **`.github/workflows/deploy-services.yml` (Main CD Workflow):**
    * **Trigger:** This workflow does not run on a `push` event. Instead, it is triggered by a **`workflow_dispatch` event**, which is initiated by the `build-python-services.yml` CI workflow upon a successful build. This controlled triggering mechanism ensures that the CD pipeline only runs after all CI checks have passed.
    * **Runner Strategy:** This is a core feature of the project's CD strategy. The deployment jobs in this workflow run on a **self-hosted GitHub Actions runner** within the project's K3s cluster. This allows the workflow to have direct, secure access to the cluster's internal network and API.
    * **Deployment Logic:** The workflow is an orchestrator. It calls the `deploy-single-service.yml` reusable workflow for each changed service.
    * **Image Promotion:** It includes a dedicated job to promote the deployed immutable SHA-tagged image to a new environment tag (e.g., `auditflow-platform/notification-service:dev`) after the Helm deployment to the cluster is verified as successful.
    * **Tooling:** This workflow's primary tool for interacting with the Kubernetes cluster is **Ansible**, which encapsulates the Helm deployment logic.

### Integrated Security Scans (DevSecOps):

Both workflows implicitly, via the reusable workflow, integrate security scanning at different stages of the CI/CD pipeline.

* **Static Application Security Testing (SAST):**
    * **Tool:** **Bandit** (`bandit -r . -ll -f sarif -o bandit_results.sarif`).
    * **Timing:** Runs *before* Docker image creation on the source code of each Python service.
    * **Output:** Generates SARIF format reports (`bandit_results.sarif`) which are then uploaded to **GitHub Code Scanning** (`github/codeql-action/upload-sarif@v3`) for centralized vulnerability management.
* **Software Composition Analysis (SCA) & Vulnerability Scanning:**
    * **Tool:** **Trivy** (`aquasecurity/trivy-action@master`).
    * **Types of Scans:**
        * **Filesystem Scan (`scan-type: 'fs'`):** Scans the project's dependencies (e.g., `requirements.txt`, `pyproject.toml`) for known vulnerabilities (`vuln`) and secrets *before* image build.
        * **Image Scan (`image-ref`):** Scans the *built Docker image* for vulnerabilities in OS packages, programming language dependencies, and misconfigurations (`vuln,secret,misconfig`).
    * **Output:** Generates SARIF format reports (e.g., `trivy-dependency-results_*.sarif`, `trivy-image-results_*.sarif`) which are uploaded to **GitHub Code Scanning**, providing a comprehensive view of supply chain and image-level security.
    * **Non-Blocking:** Scans are configured with `exit-code: '0'` to avoid failing the build immediately, allowing security results to be collected even if issues are found, which can be reviewed in GitHub's security tab.

This robust CI/CD setup ensures that code changes are continuously integrated, built into multi-architecture Docker images, and thoroughly scanned for security vulnerabilities before deployment.

---
## Kubernetes Base Configuration Details (k8s/base) ‚ò∏Ô∏è

The `k8s/base` directory contains the foundational Kubernetes YAML manifests for the **AuditFlow Platform's microservices** and their supporting infrastructure (PostgreSQL, RabbitMQ, Redis). These files provide the core, un-customized declarations for Deployments, Services, StatefulSets, and PersistentVolumeClaims. While `k8s/charts` and `k8s/overlays` are planned for future development to handle templating and environment-specific customizations, `k8s/base` serves as the initial, declarative setup.

### Core Principles and Object Types:

* **Deployments**: Define and manage the desired state for the application microservices (e.g., `audit-event-generator`, `audit-log-analysis`, `event-audit-dashboard`, `notification-service`), ensuring a specified number of replicas are running.
* **Services**:
    * **`ClusterIP`**: Used for internal communication between services within the Kubernetes cluster. Most application and infrastructure services (e.g., `rabbitmq-service`, `postgres-service`, `audit-log-analysis-service`) are exposed via `ClusterIP`.
    * **`NodePort`**: The `event-audit-dashboard-service` is exposed as a `NodePort` (specifically `30080`), allowing external access to the dashboard via any Kubernetes node's IP address.
* **StatefulSets**: Used for stateful applications like PostgreSQL, ensuring stable network identities and ordered, graceful deployment/scaling.
* **PersistentVolumeClaims (PVCs)**: Request and manage persistent storage for stateful applications (e.g., `postgres-pv-claim` for PostgreSQL), ensuring data durability across pod restarts or redeployments. Uses the `local-path` storage class, typical for K3s local storage.
* **Resource Management**: All Deployments include `requests` and `limits` for CPU and Memory, promoting efficient resource utilization and preventing resource contention.
* **Health Checks**: **Liveness and Readiness Probes** are configured for critical application services, enhancing resilience by ensuring Kubernetes routes traffic only to healthy, ready instances and restarts unhealthy ones.
* **Environment Variables**: Service-to-service communication configurations (like `RABBITMQ_HOST`, `REDIS_HOST`, `PG_HOST`) are injected into containers via environment variables, referencing the internal Kubernetes service names.
* **Secret Management**: Sensitive information, such as PostgreSQL database credentials (`PG_DB`, `PG_USER`, `PG_PASSWORD`), is securely referenced from a **Kubernetes Secret** (e.g., `postgres-credentials`), adhering to security best practices.
* **Observability Hooks**: Services include specific labels (`prometheus_scrape: "true"`) and annotations (`prometheus.io/scrape`, `prometheus.io/port`) to facilitate automatic discovery and scraping of metrics by **Prometheus**, indicating a strong focus on monitoring.

***
### Components in `k8s/base`:

* **`audit-event-generator`**: Deployment and ClusterIP Service. Generates events and connects to RabbitMQ.
* **`audit-log-analysis`**: Deployment and ClusterIP Service. Consumes events from RabbitMQ and interacts with Redis.
* **`event-audit-dashboard`**: Deployment and NodePort Service. Provides the web UI and connects to the Notification Service.
* **`notification-service`**: Deployment and ClusterIP Service. Handles alerts, connects to RabbitMQ and PostgreSQL. Securely retrieves PostgreSQL credentials from a Secret.
* **`postgres`**: PersistentVolumeClaim, ClusterIP Service, and StatefulSet. Provides a durable PostgreSQL database instance.
* **`rabbitmq`**: Deployment and ClusterIP Service. Acts as the central message broker for inter-service communication.
* **`redis`**: Deployment and ClusterIP Service. Serves as a key-value store, currently configured with `emptyDir` for non-persistent storage.

---
## Monitoring & Observability Strategy üìà
Currently, the primary focus for observability is on Prometheus metrics scraping, indicated by labels and annotations in the Kubernetes base manifests. Each Python microservice is expected to expose a `/metrics` endpoint (e.g., on port 8000 or 8001) for Prometheus to collect time-series data.

**Future Enhancements (Planned/To Be Detailed):**
* **Logging Strategy:** Details on structured logging, centralized log aggregation (e.g., ELK stack, Grafana Loki), and the implementation of correlation IDs for tracing requests across services are planned.
* **Alerting:** Configuration for Prometheus Alertmanager rules and notification channels will be added.
* **Distributed Tracing:** Future consideration for implementing distributed tracing (e.g., OpenTelemetry, Jaeger) to visualize inter-service request flows.

---

## Error Handling & Resilience Patterns üõ°Ô∏è
As a microservices architecture, the system's resilience is critical. While specific patterns like retries and circuit breakers are not explicitly detailed in the current configuration files, these are essential considerations for a robust distributed system.

**Future Enhancements (Planned/To Be Detailed):**
* **Inter-service Communication Failures:** Specific strategies for handling transient network errors, including retries with exponential backoff.
* **Circuit Breakers/Bulkheads:** Implementation details for these patterns to prevent cascading failures.
* **Graceful Degradation:** How the system will behave when dependent services are unavailable.
* **Database Error Handling:** Mechanisms for managing database connection errors, transaction failures, and retries.

---

## API Endpoints & Communication Protocols üîó
The core inter-service communication relies heavily on **RabbitMQ** for an event-driven architecture, enabling asynchronous message exchange between components.

**Current Knowns:**
* **Internal Communication:** Services communicate via RabbitMQ queues (e.g., `audit_events`, `audit_alerts`).
* **RESTful APIs:** The `event-audit-dashboard` is a web application, implying it exposes HTTP endpoints, and the `notification-service` is noted to have a future API (port 8000).

**Future Enhancements (Planned/To Be Detailed):**
* **Detailed API Endpoints:** Specific RESTful API endpoints exposed by services, their methods, and expected request/response formats.
* **API Documentation:** Whether OpenAPI/Swagger definitions are used or planned for internal/external APIs.
* **Other Protocols:** If other communication protocols (e.g., gRPC) are used for specific inter-service interactions.

---

## Deployment Strategy & Rollbacks üîÑ
The deployment process relies on GitHub Actions to orchestrate the build and a self-hosted runner to execute the deployment logic via Ansible.

**Current Knowns:**
* **Initial Deployment:** The K3s cluster is provisioned by Ansible. Application deployment is performed by the `deploy_k8s_services.yml` Ansible playbook, which calls `helm upgrade --install` using the chart and a dynamically generated override file.
* **Image Tagging:** Services are built as multi-architecture Docker images tagged with an immutable Git SHA.
* **Environment Tags & Promotion:** After a successful deployment to `dev` or `staging`, a separate job promotes the immutable SHA tag to an environment-specific tag (e.g., `notification-service:dev`) on Docker Hub.
* **Rolling Updates:** Kubernetes Deployments, by default, employ a rolling update strategy when the `template` (e.g., the image tag) changes.
* **Manual Rollback Capability:** The **Helm** package manager provides a built-in mechanism for manual rollbacks to a previous deployment version (`helm rollback <release-name>`).

**Future Enhancements (Planned/To Be Detailed):**
* **Automated Rollback Mechanism:** Specific strategies and tooling (e.g., leveraging Helm's rollback capabilities) for quickly reverting to a previous stable version in case of a failed deployment.
* **Canary Deployments/Blue-Green Deployments:** Advanced deployment strategies for minimizing risk during new releases.
* **Traffic Management:** How ingress controllers, service meshes, or other tools manage traffic during deployments or for A/B testing.

---

## Database Schema & Migrations üóÑÔ∏è
The database schema and any updates are currently managed directly within the **application codebase** of the services that interact with PostgreSQL (primarily the `notification-service`). There is no separate database migration tool (like Alembic, Flyway, or Liquibase) explicitly mentioned or configured at this stage.

**Further Understanding:** To understand the specific schema and its evolution, a review of the database interaction code within the relevant application services (e.g., ORM models, SQL statements) would be necessary.

---

## Application Details

### Audit Event Generator Service (src/audit-event-generator) ‚öôÔ∏è
* **Role**: The primary source of simulated audit events for the AuditFlow Platform.
* **Functionality**: Continuously generates diverse audit events (e.g., user logins, file modifications) and publishes them to the `audit_events` RabbitMQ queue. It also provides an API endpoint (`/generate_event`) for on-demand event creation.
* **Output**: Produces raw audit event messages into the `audit_events` RabbitMQ queue.
* **Dependencies**: Primarily relies on **RabbitMQ** for event publishing.
* **Observability**: Exposes `/healthz` and `/metrics` endpoints for health checks and Prometheus integration.
* **Detailed Context**: For in-depth information, refer to [src/audit-event-generator/CONTEXT.md](src/audit-event-generator/CONTEXT.md).
---
### Audit Log Analysis Service (src/audit-log-analysis) üïµÔ∏è

* **Role**: A critical backend service that **consumes raw audit events** from the `audit_events` RabbitMQ queue.
* **Functionality**: Performs **real-time analysis** on these events (e.g., detecting failed login bursts, sensitive file modifications) and **generates alerts**.
* **Output**: Publishes detected **alerts** to a separate `audit_alerts` RabbitMQ queue for downstream services like the Notification Service.
* **Dependencies**: Relies on **RabbitMQ** for event consumption and alert publishing, and **Redis** for stateful analysis (e.g., tracking login attempts).
* **Observability**: Exposes `/healthz` and `/metrics` endpoints for health monitoring and Prometheus integration.
* **Detailed Context**: For in-depth information, refer to [src/audit-log-analysis/CONTEXT.md](src/audit-log-analysis/CONTEXT.md).
---
### Event Audit Dashboard Service (src/event-audit-dashboard) üñ•Ô∏è
* **Role:** The frontend web application of the AuditFlow Platform.
* **Functionality:** Provides a user interface to visualize audit alerts. It fetches recent alerts from the `Notification Service` API and displays them on a dashboard, also offering detailed views for individual alerts.
* **Output:** Presents a browser-based dashboard for human consumption.
* **Dependencies:** Primarily relies on the Notification Service (via HTTP API calls) to retrieve alert data.
* **Observability:** Exposes a `/healthz` endpoint for health checks.
* **Deployment:** Uses Gunicorn for production readiness.
* **Detailed Context:** For in-depth information, refer to src/event-audit-dashboard/CONTEXT.md.
---
### Notification Service (src/notification-service) üîî
* **Role:** The central backend service for **storing and retrieving audit alerts**.
* **Functionality:** Consumes **alert messages from RabbitMQ**, persists them into a **PostgreSQL database**, and exposes a **RESTful API** for other services (like the dashboard) to query these alerts.
* **Output:** Stores alert data in PostgreSQL and serves it via an API.
* **Dependencies:** Relies on **RabbitMQ** (for alert ingestion) and **PostgreSQL** (for data storage).
* **Observability:** Exposes a `/healthz` endpoint for health checks and API endpoints for alert retrieval.
* **Deployment:** Built with **Quart** and served by **Uvicorn** for asynchronous, production-grade performance.
* **Detailed Context:** For in-depth information, refer to src/notification-service/CONTEXT.md.

---

## üß™ Testing Philosophy & Strategy Summary

Our approach to unit and integration testing is guided by practical experience, long-term maintainability, and insights from the broader development community. Here‚Äôs a summary of key takeaways and how they shape our testing practices:

---

### üîπ General Guidelines

- **No arbitrary coverage targets** (e.g. 100%) ‚Äî coverage is not a guarantee of quality.
- Focus on **high-value coverage**: business logic, edge cases, data validation, and workflows with real-world consequences.
- Prioritize **breadth over depth**: it's better to have all files partially tested than a few files exhaustively covered.

---

### üîπ Unit Testing

- Use unit tests to verify **core logic**, **critical transformations**, and **small testable functions**.
- Avoid over-testing trivial code (getters, boilerplate, etc.).
- Keep tests meaningful and easy to understand ‚Äî they should explain what the code does and catch regressions early.

---

### üîπ Integration Testing

- Integration tests simulate how components interact under real conditions.
- Ideal for:
  - HTTP routes and controllers
  - Database interactions
  - Service boundaries
- Integration tests often provide more realistic coverage than isolated unit tests.

---

### üîπ On Test-Driven Development (TDD)

- TDD can help guide design and ensure correctness, especially in complex or evolving modules.
- However, strict TDD can lead to **test bloat** and **poor design** when followed dogmatically.
- Use it **when it adds value**, not as a mandatory rule.

---

### üîπ Coverage Philosophy

- **Test what matters**, not just what exists.
- Treat coverage as a **feedback tool**, not a goal.
- Use integration and system tests to validate behavior from the user's point of view.
- Maintain tests that **prevent regressions**, not tests that just exercise lines of code.

---

### üß† Summary Quote

> ‚ÄúThe right number of tests is not a number ‚Äî it's the set of tests that gives you confidence to ship and change code safely.‚Äù

---
